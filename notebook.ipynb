{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Efficiency in Gold Mining\n",
    "\n",
    "## Project Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import warnings\n",
    "\n",
    "# Import Third Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    try: # Try to load the data locally from the data folder\n",
    "        data = pd.read_csv(f'data/{file_name}')\n",
    "    except: # Read the data from the TripTen Hub\n",
    "        data = pd.read_csv(f'/datasets/{file_name}')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_full = load_data('gold_recovery_full.csv')\n",
    "data_train = load_data('gold_recovery_train.csv')\n",
    "data_test = load_data('gold_recovery_test.csv')\n",
    "\n",
    "# Name the dataframes\n",
    "data_full.name = 'data_full'\n",
    "data_train.name = 'data_train'\n",
    "data_test.name = 'data_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data sets\n",
    "data_sets = [data_full, data_train, data_test]\n",
    "\n",
    "# Define the target columns\n",
    "target_cols = ['rougher.output.recovery', 'final.output.recovery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the date columns to datetime\n",
    "# for data in data_sets:\n",
    "#     data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that recovery is calculated correctly.  \n",
    "Using the training set, calculate recovery for the rougher.output.recovery feature. Find the MAE between your calculations and the feature values. Provide findings.\n",
    "\n",
    "Recovery = (share of gold in the concentrate right after flotation * (share of gold in the feed before flotation - share of gold in the rougher tails right after flotation)) /\n",
    "(share of gold in feed before flotation * (share of gold in concentrate right after flotation - share of gold in the rougher tails right after floatation))  \n",
    "\\* 100\n",
    "\n",
    "--------\n",
    "Recovery = (rougher.output.concentrate_au * (rougher.input.feed_au - rougher.output.tail_au)) /  \n",
    "(rougher.input.feed_au * (rougher.output.concentrate_au - rougher.output.tail_au))  \n",
    "\\* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rough_recovery(row):\n",
    "\n",
    "    C = row['rougher.output.concentrate_au']\n",
    "    F = row['rougher.input.feed_au']\n",
    "    T = row['rougher.output.tail_au']\n",
    "    if (F*(C-T)) == 0:\n",
    "        recovery = 0\n",
    "    else:\n",
    "        recovery = ((C*(F-T))/(F*(C-T)))*100\n",
    "\n",
    "    return recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rough_recovery(df):\n",
    "    \n",
    "    # Narrow the df to the necessary columns\n",
    "    df = df[['rougher.output.concentrate_au', 'rougher.input.feed_au', 'rougher.output.tail_au', 'rougher.output.recovery']]\n",
    "    \n",
    "    # Drop rows with NaN values because we can only calculate and compare the recovery if we have all the necessary values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Calculate the recovery\n",
    "    df.loc[:, 'calculated_recovery'] = df.apply(calc_rough_recovery, axis=1)\n",
    "    \n",
    "    # Calculate the mean absolute error\n",
    "    mae = np.abs(df['rougher.output.recovery'] - df['calculated_recovery']).mean()\n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for the rougher.output.recovery column in the data_train dataframe is 9.210911277458828e-15\n"
     ]
    }
   ],
   "source": [
    "print(f'The MAE for the rougher.output.recovery column in the {data_train.name} dataframe is {compare_rough_recovery(data_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE between the recorded value for rougher.output.recovery and the calculated amount is very small, therefore we can trust values in this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the features not available in the test set.\n",
    "What are these parameters?  \n",
    "What is their type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full column count: 87\n",
      "data_train column count: 87\n",
      "data_test column count: 53\n"
     ]
    }
   ],
   "source": [
    "# Print column count for each dataset\n",
    "for df in data_sets:\n",
    "    print(f'{df.name} column count: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset is missing 34 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the full and train datasets have the same columns\n",
    "assert data_full.columns.tolist() == data_train.columns.tolist()\n",
    "\n",
    "# Assert that data_full has the same number of rows as data_train + data_test\n",
    "assert data_full.shape[0] == data_train.shape[0] + data_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final.output.concentrate_ag type: float64\n",
      "final.output.concentrate_au type: float64\n",
      "final.output.concentrate_pb type: float64\n",
      "final.output.concentrate_sol type: float64\n",
      "final.output.recovery type: float64\n",
      "final.output.tail_ag type: float64\n",
      "final.output.tail_au type: float64\n",
      "final.output.tail_pb type: float64\n",
      "final.output.tail_sol type: float64\n",
      "primary_cleaner.output.concentrate_ag type: float64\n",
      "primary_cleaner.output.concentrate_au type: float64\n",
      "primary_cleaner.output.concentrate_pb type: float64\n",
      "primary_cleaner.output.concentrate_sol type: float64\n",
      "primary_cleaner.output.tail_ag type: float64\n",
      "primary_cleaner.output.tail_au type: float64\n",
      "primary_cleaner.output.tail_pb type: float64\n",
      "primary_cleaner.output.tail_sol type: float64\n",
      "rougher.calculation.au_pb_ratio type: float64\n",
      "rougher.calculation.floatbank10_sulfate_to_au_feed type: float64\n",
      "rougher.calculation.floatbank11_sulfate_to_au_feed type: float64\n",
      "rougher.calculation.sulfate_to_au_concentrate type: float64\n",
      "rougher.output.concentrate_ag type: float64\n",
      "rougher.output.concentrate_au type: float64\n",
      "rougher.output.concentrate_pb type: float64\n",
      "rougher.output.concentrate_sol type: float64\n",
      "rougher.output.recovery type: float64\n",
      "rougher.output.tail_ag type: float64\n",
      "rougher.output.tail_au type: float64\n",
      "rougher.output.tail_pb type: float64\n",
      "rougher.output.tail_sol type: float64\n",
      "secondary_cleaner.output.tail_ag type: float64\n",
      "secondary_cleaner.output.tail_au type: float64\n",
      "secondary_cleaner.output.tail_pb type: float64\n",
      "secondary_cleaner.output.tail_sol type: float64\n"
     ]
    }
   ],
   "source": [
    "# Find the columns that are missing in the test dataset\n",
    "missing_cols = data_train.columns.difference(data_test.columns).tolist()\n",
    "\n",
    "# Print the types of the columns that are missing in the test dataset\n",
    "for col in missing_cols:\n",
    "    print(f'{col} type: {data_train[col].dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the expected missing target values (rougher.output.recovery and final.output.recovery), the test data set is missing 32 other columns. These values are not known at the time of prediction, and should therefor be dropped from the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full missing value count:\n",
      "36587\n",
      "data_train missing value count:\n",
      "30320\n",
      "data_test missing value count:\n",
      "2360\n"
     ]
    }
   ],
   "source": [
    "# print missing value count for each data set\n",
    "for df in data_sets:\n",
    "    print(f'{df.name} missing value count:')\n",
    "    print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train missing value percentage:\n",
      "date                                                   0.000000\n",
      "final.output.concentrate_ag                            0.427046\n",
      "final.output.concentrate_pb                            0.427046\n",
      "final.output.concentrate_sol                           2.194543\n",
      "final.output.concentrate_au                            0.421115\n",
      "final.output.recovery                                  9.021352\n",
      "final.output.tail_ag                                   0.391459\n",
      "final.output.tail_pb                                   1.085409\n",
      "final.output.tail_sol                                  0.860024\n",
      "final.output.tail_au                                   0.391459\n",
      "primary_cleaner.input.sulfate                          7.752076\n",
      "primary_cleaner.input.depressant                       7.485172\n",
      "primary_cleaner.input.feed_size                        0.000000\n",
      "primary_cleaner.input.xanthate                         5.842230\n",
      "primary_cleaner.output.concentrate_ag                  0.486358\n",
      "primary_cleaner.output.concentrate_pb                  2.123369\n",
      "primary_cleaner.output.concentrate_sol                 3.772242\n",
      "primary_cleaner.output.concentrate_au                  0.486358\n",
      "primary_cleaner.output.tail_ag                         0.492289\n",
      "primary_cleaner.output.tail_pb                         0.587189\n",
      "primary_cleaner.output.tail_sol                        1.666667\n",
      "primary_cleaner.output.tail_au                         0.492289\n",
      "primary_cleaner.state.floatbank8_a_air                 0.237248\n",
      "primary_cleaner.state.floatbank8_a_level               0.195730\n",
      "primary_cleaner.state.floatbank8_b_air                 0.237248\n",
      "primary_cleaner.state.floatbank8_b_level               0.160142\n",
      "primary_cleaner.state.floatbank8_c_air                 0.225386\n",
      "primary_cleaner.state.floatbank8_c_level               0.160142\n",
      "primary_cleaner.state.floatbank8_d_air                 0.231317\n",
      "primary_cleaner.state.floatbank8_d_level               0.160142\n",
      "rougher.calculation.sulfate_to_au_concentrate          0.160142\n",
      "rougher.calculation.floatbank10_sulfate_to_au_feed     0.160142\n",
      "rougher.calculation.floatbank11_sulfate_to_au_feed     0.160142\n",
      "rougher.calculation.au_pb_ratio                        7.366548\n",
      "rougher.input.feed_ag                                  0.486358\n",
      "rougher.input.feed_pb                                  1.352313\n",
      "rougher.input.feed_rate                                3.042705\n",
      "rougher.input.feed_size                                2.473310\n",
      "rougher.input.feed_sol                                 1.731910\n",
      "rougher.input.feed_au                                  0.492289\n",
      "rougher.input.floatbank10_sulfate                      6.192171\n",
      "rougher.input.floatbank10_xanthate                     2.052195\n",
      "rougher.input.floatbank11_sulfate                      3.695136\n",
      "rougher.input.floatbank11_xanthate                    11.293001\n",
      "rougher.output.concentrate_ag                          0.486358\n",
      "rougher.output.concentrate_pb                          0.486358\n",
      "rougher.output.concentrate_sol                         0.960854\n",
      "rougher.output.concentrate_au                          0.486358\n",
      "rougher.output.recovery                               15.260973\n",
      "rougher.output.tail_ag                                13.345196\n",
      "rougher.output.tail_pb                                 0.486358\n",
      "rougher.output.tail_sol                               13.339265\n",
      "rougher.output.tail_au                                13.339265\n",
      "rougher.state.floatbank10_a_air                        0.314353\n",
      "rougher.state.floatbank10_a_level                      0.314353\n",
      "rougher.state.floatbank10_b_air                        0.314353\n",
      "rougher.state.floatbank10_b_level                      0.314353\n",
      "rougher.state.floatbank10_c_air                        0.314353\n",
      "rougher.state.floatbank10_c_level                      0.272835\n",
      "rougher.state.floatbank10_d_air                        0.344009\n",
      "rougher.state.floatbank10_d_level                      0.302491\n",
      "rougher.state.floatbank10_e_air                        3.576512\n",
      "rougher.state.floatbank10_e_level                      0.302491\n",
      "rougher.state.floatbank10_f_air                        0.344009\n",
      "rougher.state.floatbank10_f_level                      0.344009\n",
      "secondary_cleaner.output.tail_ag                       0.498221\n",
      "secondary_cleaner.output.tail_pb                       0.569395\n",
      "secondary_cleaner.output.tail_sol                     11.779359\n",
      "secondary_cleaner.output.tail_au                       0.486358\n",
      "secondary_cleaner.state.floatbank2_a_air               2.153025\n",
      "secondary_cleaner.state.floatbank2_a_level             0.646501\n",
      "secondary_cleaner.state.floatbank2_b_air               0.919336\n",
      "secondary_cleaner.state.floatbank2_b_level             0.664294\n",
      "secondary_cleaner.state.floatbank3_a_air               0.575326\n",
      "secondary_cleaner.state.floatbank3_a_level             0.670225\n",
      "secondary_cleaner.state.floatbank3_b_air               0.640569\n",
      "secondary_cleaner.state.floatbank3_b_level             0.652432\n",
      "secondary_cleaner.state.floatbank4_a_air               0.765125\n",
      "secondary_cleaner.state.floatbank4_a_level             0.670225\n",
      "secondary_cleaner.state.floatbank4_b_air               0.545670\n",
      "secondary_cleaner.state.floatbank4_b_level             0.551601\n",
      "secondary_cleaner.state.floatbank5_a_air               0.504152\n",
      "secondary_cleaner.state.floatbank5_a_level             0.504152\n",
      "secondary_cleaner.state.floatbank5_b_air               0.504152\n",
      "secondary_cleaner.state.floatbank5_b_level             0.498221\n",
      "secondary_cleaner.state.floatbank6_a_air               0.610913\n",
      "secondary_cleaner.state.floatbank6_a_level             0.504152\n",
      "dtype: float64\n",
      "\n",
      "data_test missing value percentage:\n",
      "date                                          0.000000\n",
      "primary_cleaner.input.sulfate                 5.157104\n",
      "primary_cleaner.input.depressant              4.849727\n",
      "primary_cleaner.input.feed_size               0.000000\n",
      "primary_cleaner.input.xanthate                2.834699\n",
      "primary_cleaner.state.floatbank8_a_air        0.273224\n",
      "primary_cleaner.state.floatbank8_a_level      0.273224\n",
      "primary_cleaner.state.floatbank8_b_air        0.273224\n",
      "primary_cleaner.state.floatbank8_b_level      0.273224\n",
      "primary_cleaner.state.floatbank8_c_air        0.273224\n",
      "primary_cleaner.state.floatbank8_c_level      0.273224\n",
      "primary_cleaner.state.floatbank8_d_air        0.273224\n",
      "primary_cleaner.state.floatbank8_d_level      0.273224\n",
      "rougher.input.feed_ag                         0.273224\n",
      "rougher.input.feed_pb                         0.273224\n",
      "rougher.input.feed_rate                       0.683060\n",
      "rougher.input.feed_size                       0.375683\n",
      "rougher.input.feed_sol                        1.144126\n",
      "rougher.input.feed_au                         0.273224\n",
      "rougher.input.floatbank10_sulfate             4.388661\n",
      "rougher.input.floatbank10_xanthate            2.100410\n",
      "rougher.input.floatbank11_sulfate             0.939208\n",
      "rougher.input.floatbank11_xanthate            6.028005\n",
      "rougher.state.floatbank10_a_air               0.290301\n",
      "rougher.state.floatbank10_a_level             0.273224\n",
      "rougher.state.floatbank10_b_air               0.290301\n",
      "rougher.state.floatbank10_b_level             0.273224\n",
      "rougher.state.floatbank10_c_air               0.290301\n",
      "rougher.state.floatbank10_c_level             0.273224\n",
      "rougher.state.floatbank10_d_air               0.290301\n",
      "rougher.state.floatbank10_d_level             0.273224\n",
      "rougher.state.floatbank10_e_air               0.290301\n",
      "rougher.state.floatbank10_e_level             0.273224\n",
      "rougher.state.floatbank10_f_air               0.290301\n",
      "rougher.state.floatbank10_f_level             0.273224\n",
      "secondary_cleaner.state.floatbank2_a_air      0.341530\n",
      "secondary_cleaner.state.floatbank2_a_level    0.273224\n",
      "secondary_cleaner.state.floatbank2_b_air      0.392760\n",
      "secondary_cleaner.state.floatbank2_b_level    0.273224\n",
      "secondary_cleaner.state.floatbank3_a_air      0.580601\n",
      "secondary_cleaner.state.floatbank3_a_level    0.273224\n",
      "secondary_cleaner.state.floatbank3_b_air      0.273224\n",
      "secondary_cleaner.state.floatbank3_b_level    0.273224\n",
      "secondary_cleaner.state.floatbank4_a_air      0.273224\n",
      "secondary_cleaner.state.floatbank4_a_level    0.273224\n",
      "secondary_cleaner.state.floatbank4_b_air      0.273224\n",
      "secondary_cleaner.state.floatbank4_b_level    0.273224\n",
      "secondary_cleaner.state.floatbank5_a_air      0.273224\n",
      "secondary_cleaner.state.floatbank5_a_level    0.273224\n",
      "secondary_cleaner.state.floatbank5_b_air      0.273224\n",
      "secondary_cleaner.state.floatbank5_b_level    0.273224\n",
      "secondary_cleaner.state.floatbank6_a_air      0.273224\n",
      "secondary_cleaner.state.floatbank6_a_level    0.273224\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the percentage of missing values for each column in the two data subsets\n",
    "pd.set_option('display.max_rows', None)\n",
    "for df in data_sets[1:]:\n",
    "    print(f'{df.name} missing value percentage:')\n",
    "    print(df.isnull().mean()*100)\n",
    "    print()\n",
    "    \n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train rows with NaN values: 34.66%\n",
      "data_test rows with NaN values: 8.08%\n"
     ]
    }
   ],
   "source": [
    "# Get percent of rows with NaN values in any column for the two data subsets\n",
    "for df in data_sets[1:]:\n",
    "    print(f'{df.name} rows with NaN values: {df.isnull().any(axis=1).mean()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full rows with NaN values in target columns: 14.43%\n"
     ]
    }
   ],
   "source": [
    "# Print percent of rows with NaN values in target columns in the full dataset\n",
    "print(f'{data_full.name} rows with NaN values in target columns: {data_full[target_cols].isnull().any(axis=1).mean()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are significant percentages of missing values across both axises. Dropping nearly 35% of rows in the training data is clearly not a viable option. Dropping columns is also not an ideal solution, since we want as many features as possible for prediction (and we would still have plenty of rows with missing values). Thankfully, we know from the project description that \"Parameters that are next to each other in terms of time are often similar.\", therefor we can forward fill missing feature values. We will, however, have to drop rows with missing values in either target, which accounts for about 14% of the total data (we want to train our model only on actual results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Based on the discoveries from exploring the data, the following steps will be taken to preprocess the data for the task at hand:\n",
    "\n",
    "1. Drop rows where either target is missing from full dataset.\n",
    "2. Be sure full dataset is organized by date, then forward fill missing values (because \"Parameters that are next to each other in terms of time are often similar.\"). Check no missing values.\n",
    "3. Redefine the test df and train df as inner merge on date with full data set, drop necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the target columns\n",
    "data_full = data_full.dropna(subset=target_cols)\n",
    "\n",
    "# Sort full dataset by date\n",
    "data_full = data_full.sort_values('date')\n",
    "# Forward fill the missing values\n",
    "data_full = data_full.fillna(method='ffill')\n",
    "# Assert no missing values\n",
    "assert data_full.isnull().sum().sum() == 0\n",
    "\n",
    "\n",
    "# Define the drop columns\n",
    "drop_cols = [col for col in missing_cols if col not in target_cols]\n",
    "\n",
    "# Redefine the train data set as inner join of train dates and full data\n",
    "data_train_dates = data_train['date'].to_frame()\n",
    "data_train = data_train_dates.merge(data_full, on='date', how='inner')\n",
    "# Drop the drop columns\n",
    "data_train = data_train.drop(drop_cols, axis=1)\n",
    "\n",
    "# Redefine the test data set as inner join of test dates and full data\n",
    "data_test_dates = data_test['date'].to_frame()\n",
    "data_test = data_test_dates.merge(data_full, on='date', how='inner')\n",
    "# Drop the drop columns and target columns\n",
    "data_test = data_test.drop(drop_cols + target_cols, axis=1)\n",
    "\n",
    "# Assert no missing values in the subsets\n",
    "assert data_train.isnull().sum().sum() == 0\n",
    "assert data_test.isnull().sum().sum() == 0\n",
    "\n",
    "# Assert the data_test columns equal the data_train columns minus the target columns\n",
    "assert data_test.columns.tolist() == data_train.drop(target_cols, axis=1).columns.tolist()\n",
    "\n",
    "# Assert the data_full rows equal the data_train rows plus the data_test rows\n",
    "assert data_full.shape[0] == data_train.shape[0] + data_test.shape[0]\n",
    "\n",
    "# Name the dataframes\n",
    "data_full.name = 'data_full'\n",
    "data_train.name = 'data_train'\n",
    "data_test.name = 'data_test'\n",
    "\n",
    "data_sets = [data_full, data_train, data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full shape: (19439, 87)\n",
      "data_train shape: (14149, 55)\n",
      "data_test shape: (5290, 53)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the data sets\n",
    "for data in data_sets:\n",
    "    print(f'{data.name} shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full min date: 2016-01-15 00:00:00, max date: 2018-08-18 10:59:59\n",
      "data_train min date: 2016-01-15 00:00:00, max date: 2018-08-18 10:59:59\n",
      "data_test min date: 2016-09-01 00:59:59, max date: 2017-12-31 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# Print the min and max date for each dataset\n",
    "for df in data_sets:\n",
    "    print(f'{df.name} min date: {df[\"date\"].min()}, max date: {df[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TripTenHub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
