{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Efficiency in Gold Mining\n",
    "\n",
    "## Project Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import warnings\n",
    "\n",
    "# Import Third Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    try: # Try to load the data locally from the data folder\n",
    "        data = pd.read_csv(f'data/{file_name}')\n",
    "    except: # Read the data from the TripTen Hub\n",
    "        data = pd.read_csv(f'/datasets/{file_name}')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_full = load_data('gold_recovery_full.csv')\n",
    "data_train = load_data('gold_recovery_train.csv')\n",
    "data_test = load_data('gold_recovery_test.csv')\n",
    "\n",
    "# Name the dataframes\n",
    "data_full.name = 'data_full'\n",
    "data_train.name = 'data_train'\n",
    "data_test.name = 'data_test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data sets\n",
    "data_sets = [data_full, data_train, data_test]\n",
    "\n",
    "# Define the target columns\n",
    "target_columns = ['rougher.output.recovery', 'final.output.recovery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime\n",
    "for data in data_sets:\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that recovery is calculated correctly.  \n",
    "Using the training set, calculate recovery for the rougher.output.recovery feature. Find the MAE between your calculations and the feature values. Provide findings.\n",
    "\n",
    "Recovery = (share of gold in the concentrate right after flotation * (share of gold in the feed before flotation - share of gold in the rougher tails right after flotation)) /\n",
    "(share of gold in feed before flotation * (share of gold in concentrate right after flotation - share of gold in the rougher tails right after floatation))  \n",
    "\\* 100\n",
    "\n",
    "--------\n",
    "Recovery = (rougher.output.concentrate_au * (rougher.input.feed_au - rougher.output.tail_au)) /  \n",
    "(rougher.input.feed_au * (rougher.output.concentrate_au - rougher.output.tail_au))  \n",
    "\\* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rough_recovery(row):\n",
    "\n",
    "    C = row['rougher.output.concentrate_au']\n",
    "    F = row['rougher.input.feed_au']\n",
    "    T = row['rougher.output.tail_au']\n",
    "    recovery = ((C*(F-T))/(F*(C-T)))*100\n",
    "\n",
    "    return recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rough_recovery(df):\n",
    "    \n",
    "    # Narrow the df to the necessary columns\n",
    "    df = df[['rougher.output.concentrate_au', 'rougher.input.feed_au', 'rougher.output.tail_au', 'rougher.output.recovery']]\n",
    "    \n",
    "    \n",
    "    # Get count of rows with NaN values\n",
    "    num_rows_with_nan = df.isnull().any(axis=1).sum()\n",
    "    if num_rows_with_nan > 0:\n",
    "        warnings.warn(f'There are {num_rows_with_nan} rows with NaN values!')\n",
    "        \n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna()\n",
    "    \n",
    "    # Calculate the recovery\n",
    "    df.loc[:, 'calculated_recovery'] = df.apply(calc_rough_recovery, axis=1)\n",
    "    \n",
    "    # Calculate the mean absolute error\n",
    "    mae = np.abs(df['rougher.output.recovery'] - df['calculated_recovery']).mean()\n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for the rougher.output.recovery column in the data_train dataframe is 9.210911277458828e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/gcv0f_7x7cjd_c0nj62lnd0c0000gn/T/ipykernel_36374/114006725.py:10: UserWarning: There are 2573 rows with NaN values!\n",
      "  warnings.warn(f'There are {num_rows_with_nan} rows with NaN values!')\n"
     ]
    }
   ],
   "source": [
    "print(f'The MAE for the rougher.output.recovery column in the {data_train.name} dataframe is {compare_rough_recovery(data_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE between the recorded value for rougher.output.recovery and the calculated amount is very small, but there a lot of missing values in the dataset still. I'd like to fill these missing values and then check again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the features not available in the test set.\n",
    "What are these parameters?  \n",
    "What is their type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full column count: 87\n",
      "data_train column count: 87\n",
      "data_test column count: 53\n"
     ]
    }
   ],
   "source": [
    "# Print column count for each dataset\n",
    "for df in data_sets:\n",
    "    print(f'{df.name} column count: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset is missing 34 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the full and train datasets have the same columns\n",
    "assert data_full.columns.tolist() == data_train.columns.tolist()\n",
    "\n",
    "# Assert that data_full has the same number of rows as data_train + data_test\n",
    "assert data_full.shape[0] == data_train.shape[0] + data_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final.output.concentrate_ag type: float64\n",
      "final.output.concentrate_au type: float64\n",
      "final.output.concentrate_pb type: float64\n",
      "final.output.concentrate_sol type: float64\n",
      "final.output.recovery type: float64\n",
      "final.output.tail_ag type: float64\n",
      "final.output.tail_au type: float64\n",
      "final.output.tail_pb type: float64\n",
      "final.output.tail_sol type: float64\n",
      "primary_cleaner.output.concentrate_ag type: float64\n",
      "primary_cleaner.output.concentrate_au type: float64\n",
      "primary_cleaner.output.concentrate_pb type: float64\n",
      "primary_cleaner.output.concentrate_sol type: float64\n",
      "primary_cleaner.output.tail_ag type: float64\n",
      "primary_cleaner.output.tail_au type: float64\n",
      "primary_cleaner.output.tail_pb type: float64\n",
      "primary_cleaner.output.tail_sol type: float64\n",
      "rougher.calculation.au_pb_ratio type: float64\n",
      "rougher.calculation.floatbank10_sulfate_to_au_feed type: float64\n",
      "rougher.calculation.floatbank11_sulfate_to_au_feed type: float64\n",
      "rougher.calculation.sulfate_to_au_concentrate type: float64\n",
      "rougher.output.concentrate_ag type: float64\n",
      "rougher.output.concentrate_au type: float64\n",
      "rougher.output.concentrate_pb type: float64\n",
      "rougher.output.concentrate_sol type: float64\n",
      "rougher.output.recovery type: float64\n",
      "rougher.output.tail_ag type: float64\n",
      "rougher.output.tail_au type: float64\n",
      "rougher.output.tail_pb type: float64\n",
      "rougher.output.tail_sol type: float64\n",
      "secondary_cleaner.output.tail_ag type: float64\n",
      "secondary_cleaner.output.tail_au type: float64\n",
      "secondary_cleaner.output.tail_pb type: float64\n",
      "secondary_cleaner.output.tail_sol type: float64\n"
     ]
    }
   ],
   "source": [
    "# Find the columns that are missing in the test dataset\n",
    "missing_cols = data_train.columns.difference(data_test.columns)\n",
    "\n",
    "# Print the types of the columns that are missing in the test dataset\n",
    "for col in missing_cols:\n",
    "    print(f'{col} type: {data_train[col].dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the expected missing target values (rougher.output.recovery and final.output.recovery), the test data set is missing 32 other columns. These values are not known at the time of prediction, and should therefor be dropped from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full missing value count:\n",
      "36587\n",
      "data_train missing value count:\n",
      "30320\n",
      "data_test missing value count:\n",
      "2360\n"
     ]
    }
   ],
   "source": [
    "# print missing value count for each data set\n",
    "for df in data_sets:\n",
    "    print(f'{df.name} missing value count:')\n",
    "    print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_full min date: 2016-01-15 00:00:00, max date: 2018-08-18 10:59:59\n",
      "data_train min date: 2016-01-15 00:00:00, max date: 2018-08-18 10:59:59\n",
      "data_test min date: 2016-09-01 00:59:59, max date: 2017-12-31 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# Print the min and max date for each dataset\n",
    "for df in data_sets:\n",
    "    print(f'{df.name} min date: {df[\"date\"].min()}, max date: {df[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TripTenHub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
